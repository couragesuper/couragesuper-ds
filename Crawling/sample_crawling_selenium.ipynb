{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> CAFE24 Crawling with Selenium  </H1>\n",
    "\n",
    "1.Anaconda Selenium error \n",
    "ModuleNotFoundError: No module named 'selenium'\n",
    "  \n",
    "2.Failed - installing selenium (1)\n",
    "파이참의 동작 코드와 위와 같은 문제를 일으켜서, \n",
    "pip3 install selenium 를 수행했으나, 여전히 같았다. \n",
    "\n",
    "3.Referneces\n",
    "https://devminjun.github.io/blog/Selenium\n",
    "\n",
    "4.download chrome driver \n",
    "https://chromedriver.storage.googleapis.com/index.html?path=2.40/\n",
    "\n",
    "5.OK- installing selenium\n",
    " ~/anaconda3/bin/pip install selenium\n",
    " \n",
    "6.add permission to chromedriver\n",
    "permission 문제가 생겨서 chromedriver에 777을 부여 \n",
    "\n",
    "7.chrome을 찾지 못함\n",
    "WebDriverException: Message: unknown error: cannot find Chrome binary\n",
    "  (Driver info: chromedriver=2.40.565383 (76257d1ab79276b2d53ee976b2c3e3b9f335cde7),platform=Linux 4.4.0-127-generic x86_64)\n",
    "  \n",
    "8.installing chronium-browser\n",
    "sudo apt install chromium-browser\n",
    "이렇게 하니, 진해이 된다. 이것저것 많이도 깐다. \n",
    "\n",
    "9.using headless crawling\n",
    "headless하게 하는 방법에 대해서 설명하고 있다. \n",
    "https://beomi.github.io/2017/09/28/HowToMakeWebCrawler-Headless-Chrome/\n",
    "\n",
    "--headless 를 붙이니까, 일단은 성공하는 듯하게 보인다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import folium\n",
    "import json\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore' ,  category=FutureWarning)\n",
    "\n",
    "#references : http://selenium-python.readthedocs.io/\n",
    "\n",
    "#path_chrome_driver = \"D:\\\\Workspace\\\\temp\\\\DataScience-GitData\\\\DataScience-master\\\\chromedriver\"\n",
    "path_chrome_driver = \"/root/chromedriver/chromedriver\"\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--window-size=1920x1080')\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for resource\n",
    "import platform\n",
    "from matplotlib import font_manager,rc\n",
    "def checkFontResource()  :\n",
    "    path = \"c:\\\\Windows\\\\Fonts\\\\malgun.ttf\"\n",
    "\n",
    "    if platform.system() == \"Darwin\" :\n",
    "        rc('font', family='AppleGothic')\n",
    "    elif platform.system() == \"Windows\" :\n",
    "        font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "        rc('font', family = font_name)\n",
    "    else :\n",
    "        print(\"unknown system... sorry~~~~\")\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def crawling_sendkey( driver, by , name, sendvalue ) :\n",
    "    if( by.lower() == \"name \") : elem = driver.find_element_by_name(name)\n",
    "    elif (by.lower() == \"xpath\"): elem = driver.find_element_by_xpath(name)\n",
    "    else : return False\n",
    "    elem.clear()\n",
    "    elem.send_keys(sendvalue)\n",
    "    sleep(0.1)\n",
    "    return True;\n",
    "\n",
    "def crawling_select_getlist( driver, xpath ) :\n",
    "    list = driver.find_element_by_xpath(xpath).find_elements_by_tag_name(\"option\")\n",
    "    value = [option.text for option in list]\n",
    "    value = value[1:]  # 전체 제거?\n",
    "    sleep(0.5)  # this makes correct result\n",
    "    return value\n",
    "\n",
    "def crawling_table_getlist_tag_xpath(driver, xpath, tag , isElemList ):\n",
    "    list_webelem = driver.find_element_by_xpath(xpath).find_elements_by_tag_name(tag)\n",
    "    if isElemList == True : return list_webelem\n",
    "    else:  return [ {\"text\":elem.text, \"href\":elem.get_attribute(\"href\"), \"elem\":elem , \"src\":elem.get_attribute(\"src\")} for elem in list_webelem ]\n",
    "\n",
    "def crawling_select_Item( driver, xpath , selectId , order ) :\n",
    "    list = driver.find_element_by_xpath(xpath).find_elements_by_tag_name(\"option\")\n",
    "    value = [option.text for option in list]\n",
    "    value = value[1:]  # 전체 제거?\n",
    "    element = driver.find_element_by_id( selectId )\n",
    "    element.send_keys(value[order])\n",
    "    sleep(0.5)  # this makes correct result\n",
    "\n",
    "def crawling_click( driver , xpath  ) :\n",
    "    driver.find_element_by_xpath(xpath).click()\n",
    "    sleep(0.5)\n",
    "\n",
    "def InitWebDriver( WebDriverPath, URL , delay ) :\n",
    "    global webDriver\n",
    "    webDriver = webdriver.Chrome(WebDriverPath ,chrome_options=options)\n",
    "    webDriver.get(URL)\n",
    "    webDriver.implicitly_wait(delay)\n",
    "    return True\n",
    "\n",
    "def Main() :\n",
    "    global path_chrome_driver\n",
    "    checkFontResource()\n",
    "    #go to url with initalized webdriver\n",
    "    \n",
    "    print(\"1\")\n",
    "    \n",
    "    InitWebDriver( path_chrome_driver , \"http://www.bookcosmos.com\" , 1 )\n",
    "    #login\n",
    "    crawling_sendkey(webDriver, \"xpath\", \"/html/body/div/div[2]/div/ul[1]/li[1]/form/div/div[1]/input[1]\", \"hamtorigun\" )\n",
    "    crawling_sendkey(webDriver, \"xpath\", \"/html/body/div/div[2]/div/ul[1]/li[1]/form/div/div[1]/input[2]\", \"saturn\")\n",
    "    crawling_click(webDriver,\"//*[@id=\\\"btnLogin\\\"]\" )\n",
    "    crawling_click(webDriver, \"/html/body/div/div[2]/div/ul[1]/li[2]/a\")\n",
    "\n",
    "    sleep(0.5)\n",
    "\n",
    "    listA = []\n",
    "    listB = []\n",
    "\n",
    "    print(\"2\")\n",
    "    \n",
    "    # table 1 에서 href와 Text 얻어내기\n",
    "    #print( \"len of table1's href \", len (webDriver.find_element_by_xpath(\"/html/body/table/tbody/tr/td/table/tbody/tr[1]/td[1]/table/tbody/tr/td/table[4]/tbody/tr/td[1]/table[3]/tbody/tr[1]/td/table[2]/tbody/tr[1]/td/table[1]/tbody\").find_elements_by_tag_name(\"a\") ) )\n",
    "\n",
    "    listA = crawling_table_getlist_tag_xpath( webDriver , \"/html/body/table/tbody/tr/td/table/tbody/tr[1]/td[1]/table/tbody/tr/td/table[4]/tbody/tr/td[1]/table[3]/tbody/tr[1]/td/table[2]/tbody/tr[1]/td/table[1]\" , \"a\" , False )\n",
    "    listB = crawling_table_getlist_tag_xpath( webDriver , \"/html/body/table/tbody/tr/td/table/tbody/tr[1]/td[1]/table/tbody/tr/td/table[4]/tbody/tr/td[1]/table[3]/tbody/tr[1]/td/table[2]/tbody/tr[1]/td/table[2]\" , \"a\" , False )\n",
    "    #listA[1][\"elem\"].click()\n",
    "\n",
    "    xpath_lastPage  = \"/html/body/table/tbody/tr/td/table/tbody/tr[1]/td[1]/table/tbody/tr/td/table[4]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]/table/tbody/tr[2]/td/a[11]/img\"\n",
    "    xpath_lastPage2 = \"/html/body/table/tbody/tr/td/table/tbody/tr[1]/td[1]/table/tbody/tr/td/table[4]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]/table/tbody/tr[2]/td/a[10]/img\"\n",
    "\n",
    "    xpath_scrollBar = \"/html/body/table/tbody/tr/td/table/tbody/tr[1]/td[1]/table/tbody/tr/td/table[4]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]/table/tbody/tr[2]/td\"\n",
    "\n",
    "    if False :\n",
    "        for elem in listA :\n",
    "            print( elem )\n",
    "            webDriver.get(elem[\"href\"])\n",
    "            sleep(0.5)\n",
    "            crawling_click(webDriver, xpath_lastPage2)\n",
    "            sleep(0.5)\n",
    "        for elem in listB :\n",
    "            print(elem)\n",
    "            webDriver.get(elem[\"href\"])\n",
    "            sleep(0.5)\n",
    "            crawling_click(webDriver, xpath_lastPage2)\n",
    "            sleep(0.5)\n",
    "    else :        \n",
    "        print(\"3\")        \n",
    "        webDriver.get(listA[2][\"href\"])\n",
    "        if False :\n",
    "            crawling_click( webDriver, xpath_lastPage )\n",
    "            aList = crawling_table_getlist_tag_xpath(webDriver, xpath_scrollBar , \"a\", False)\n",
    "            strongList = crawling_table_getlist_tag_xpath(webDriver, xpath_scrollBar , \"strong\", False)\n",
    "        else :\n",
    "            imgList = crawling_table_getlist_tag_xpath(webDriver, xpath_scrollBar, \"img\", False)\n",
    "            aList = crawling_table_getlist_tag_xpath(webDriver, xpath_scrollBar, \"a\", False)\n",
    "            strongList = crawling_table_getlist_tag_xpath(webDriver, xpath_scrollBar, \"strong\", False)\n",
    "            print( imgList )\n",
    "            print( aList )\n",
    "            print( strongList ) # current cursor\n",
    "    exit(0)\n",
    "    \n",
    "Main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
